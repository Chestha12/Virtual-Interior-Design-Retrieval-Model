{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981651c8-03f6-470a-9060-9a7f98107c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, GlobalAveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from torchvision import transforms\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63a1138-9c32-4173-8960-840823751238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading BLIP model for image captioning\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
    "\n",
    "# Loading Universal Sentence Encoder (USE) for text vectorization\n",
    "use_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "# Loading images from folders\n",
    "base_path = \"Interior_images\"\n",
    "categories = [\"Bathroom\", \"Bedroom\",\"Dinning\",\"Kitchen\",\"Livingroom\"]\n",
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "for category in categories:\n",
    "    folder_path = os.path.join(base_path, category)\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "            image_paths.append(os.path.join(folder_path, filename))\n",
    "            labels.append(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f463179-57b6-4c61-9afc-135833145a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating captions using BLIP with beam search\n",
    "descriptions = []\n",
    "def generate_caption(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        output = blip_model.generate(**inputs,)\n",
    "    return processor.decode(output[0], skip_special_tokens=True).lower()\n",
    "descriptions = [generate_caption(img) for img in image_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f79240-affc-4066-a10a-32d551f964d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text descriptions to vectors using USE\n",
    "text_vectors = np.array(use_model(descriptions))\n",
    "\n",
    "np.save(\"text_vectors.npy\", text_vectors)\n",
    "print(\"Text vectors saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f51506-4d45-474f-9c13-afee1cc42391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({\"Image Path\": image_paths, \"Description\": descriptions})\n",
    "df.to_csv(\"image_descriptions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46741934-89b2-4bfa-aeb7-809fdd180443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN model for image feature extraction\n",
    "base_model = ResNet50(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
    "for layer in base_model.layers[-5:]:\n",
    "    layer.trainable = True  # Unfreeze last 5 layers for fine-tuning\n",
    "\n",
    "cnn_model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    BatchNormalization(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(128)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276fb5cd-e446-4bae-adb1-f997cafdbd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_features(image_path):\n",
    "    image = load_img(image_path, target_size=(224, 224))\n",
    "    image = img_to_array(image)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    image = tf.keras.applications.resnet50.preprocess_input(image)\n",
    "    features = cnn_model.predict(image)\n",
    "    return features.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f98cdd-519e-44ba-aca0-f4868beee019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features for all images\n",
    "image_features = np.array([extract_image_features(img) for img in image_paths])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0799e27c-abe5-46a3-bb13-fe48e6734bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train KNN model for text-based retrieval\n",
    "\n",
    "knn_text = NearestNeighbors(n_neighbors=10, metric=\"minkowski\", p = 3)\n",
    "knn_text.fit(text_vectors)\n",
    "\n",
    "knn_image = NearestNeighbors(n_neighbors=10, metric=\"cosine\")\n",
    "knn_image.fit(image_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb72c49f-14ec-4980-87f9-880d0473244b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve images based on text query\n",
    "def retrieve_images_text(query_text, category=None, max_results=5, knn_depth=10):\n",
    "    query_vector = np.array(use_model([query_text]))\n",
    "    distances, indices = knn_text.kneighbors(query_vector, n_neighbors=knn_depth)\n",
    "\n",
    "    result_images = []\n",
    "    for i in indices[0]:\n",
    "        img_path = image_paths[i]\n",
    "\n",
    "        if category and category.lower() not in img_path.lower():\n",
    "            continue  # Apply category filter\n",
    "\n",
    "        result_images.append(img_path)\n",
    "        \n",
    "        if len(result_images) >= max_results:\n",
    "            break\n",
    "\n",
    "    return result_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc8740e-5e1e-43f0-a208-9ba60b7abbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_features(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\").resize((224, 224))\n",
    "    image = np.expand_dims(np.array(image) / 255.0, axis=0)\n",
    "    features = cnn_model.predict(image)\n",
    "    return features.flatten()\n",
    "\n",
    "\n",
    "# Function to retrieve similar images based on an uploaded image\n",
    "def retrieve_images_from_image(uploaded_image, category=None, max_results=5, knn_depth=10):\n",
    "    image_vector = extract_image_features(uploaded_image)  # Convert image to vector\n",
    "    distances, indices = knn_image.kneighbors([image_vector.flatten()], n_neighbors=knn_depth)\n",
    "\n",
    "    result_images = []\n",
    "    for i in indices[0]:\n",
    "        img_path = image_paths[i]\n",
    "\n",
    "        # Ensure image belongs to the specified category\n",
    "        if category and category.lower() not in img_path.lower():\n",
    "            continue \n",
    "        \n",
    "        result_images.append(img_path)\n",
    "        \n",
    "        if len(result_images) >= max_results:\n",
    "            break\n",
    "\n",
    "    return result_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f47e7d5-4415-4735-baed-b55f1d14ae8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(image_list, title):\n",
    "    fig, axes = plt.subplots(1, len(image_list), figsize=(20, 10))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "\n",
    "    for ax, img_path in zip(axes, image_list):\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        ax.imshow(image)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
